{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"x = np.array([[1,2,3],[4,5,6]])\nprint(\"x:\\n{}\".format(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy import sparse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eye = np.eye(4)\nprint(\"Numpy array:\\n{}\".format(eye))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sparse_matrix = sparse.csr_matrix(eye)\nprint(\"\\n Sparse Matrix: \\n{}\".format(sparse_matrix))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#direct creation of sparse representation of sparse data\n\nones = np.ones(4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.ones?\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rows = np.arange(4)\ncols = np.arange(4)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eye_coo = sparse.coo_matrix((ones,(rows,cols) ))\nprint(\"COO representation: \\n{}\".format(eye_coo))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt \n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.datasets import load_iris","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = load_iris()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Keys of the dataset:{}\".format(df.keys()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Target names: {}\".format(df['target_names']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Feature names: {}\".format(df['feature_names']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Type of data: {}\".format(type(df['data'])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_test_split?","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(df['data'], df['target'], random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"X_train shape : {}\".format(X_train.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"y_train shape : {}\".format(y_train.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"iris_df = pd.DataFrame(X_train, columns = df['feature_names'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"iris_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot?","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#grr = sns.pairplot(iris_df, hue = y_train , markers = '*')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1 = iris_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1['target'] = y_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(df1, hue = 'target')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"knn.fit(X_train, y_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_new = [[5,2.9,1, 0.2]]\nprediction = knn.predict(X_new)\n\nprint(\"Prediction: {}\".format(prediction))\nprint(\"Predicted class: {}\".format(df['target_names'][prediction]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = knn.predict(X_test)\nprint(\"Test set prediction: {}\".format(y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Test score: {:.2f}\".format(np.mean(y_test==y_pred)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Test score by default checker: {:.2f}\".format(knn.score(X_test,y_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nfrom scipy import signal\nfrom sklearn.datasets import load_boston\nfrom sklearn.preprocessing import MinMaxScaler, PolynomialFeatures\nfrom sklearn.datasets import make_blobs\n\n#DATA_PATH = os.path.join(os.path.dirname(__file__), \"..\", \"data\")\n\n\ndef make_forge():\n    # a carefully hand-designed dataset lol\n    X, y = make_blobs(centers=2, random_state=4, n_samples=30)\n    y[np.array([7, 27])] = 0\n    mask = np.ones(len(X), dtype=np.bool)\n    mask[np.array([0, 1, 5, 26])] = 0\n    X, y = X[mask], y[mask]\n    return X, y\n\n\ndef make_wave(n_samples=100):\n    rnd = np.random.RandomState(42)\n    x = rnd.uniform(-3, 3, size=n_samples)\n    y_no_noise = (np.sin(4 * x) + x)\n    y = (y_no_noise + rnd.normal(size=len(x))) / 2\n    return x.reshape(-1, 1), y\n\n\ndef load_extended_boston():\n    boston = load_boston()\n    X = boston.data\n\n    X = MinMaxScaler().fit_transform(boston.data)\n    X = PolynomialFeatures(degree=2, include_bias=False).fit_transform(X)\n    return X, boston.target\n\n\ndef load_citibike():\n    data_mine = pd.read_csv(os.path.join(DATA_PATH, \"citibike.csv\"))\n    data_mine['one'] = 1\n    data_mine['starttime'] = pd.to_datetime(data_mine.starttime)\n    data_starttime = data_mine.set_index(\"starttime\")\n    data_resampled = data_starttime.resample(\"3h\").sum().fillna(0)\n    return data_resampled.one\n\n\ndef make_signals():\n    # fix a random state seed\n    rng = np.random.RandomState(42)\n    n_samples = 2000\n    time = np.linspace(0, 8, n_samples)\n    # create three signals\n    s1 = np.sin(2 * time)  # Signal 1 : sinusoidal signal\n    s2 = np.sign(np.sin(3 * time))  # Signal 2 : square signal\n    s3 = signal.sawtooth(2 * np.pi * time)  # Signal 3: saw tooth signal\n\n    # concatenate the signals, add noise\n    S = np.c_[s1, s2, s3]\n    S += 0.2 * rng.normal(size=S.shape)\n\n    S /= S.std(axis=0)  # Standardize data\n    S -= S.min()\n    return S","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nX,y = make_forge()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (12,8))\nsns.scatterplot(x = X[:,0], y = X[:,1], hue = y)\nplt.legend([\"Class 0\", \"Class 1\"], loc=4)\nplt.xlabel(\"First feature\")\nplt.ylabel(\"Second feature\")\nprint(\"X.shape: {}\".format(X.shape))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X1, y1 = make_wave(n_samples = 40)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.scatterplot(X1[:,0],y1, marker = '+')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot?","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.datasets import load_breast_cancer\ncancer = load_breast_cancer()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(type(cancer))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Cancer keys: \\n{}\".format(cancer.keys()))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = cancer['data']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = pd.DataFrame(X, columns = cancer['feature_names'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = pd.DataFrame(cancer['target'],columns = ['target'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.merge?","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cancer_df = X","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cancer_df['target'] = y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cancer_df.groupby(by = 'target').count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX,y = make_forge()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X,y, random_state= 0)\n  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nclf = KNeighborsClassifier()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Predictions: \\n{}\".format(clf.predict(X_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Test set accuracy:\\n {:.2f}\".format(clf.score(X_test, y_test))) #since we're passing X_test into this .score function it means it is\n                                                                        #first calculating y_predicted, then comparing y_pred with y_test to calculate score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clf.score?","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# fig, axes = plt.subplots(1,3, figsize = (10,3)\nn_neighbors = 3\nh = 0.01\nfrom matplotlib.colors import ListedColormap\ncmap_light = ListedColormap(['orange', 'cyan', 'cornflowerblue'])\ncmap_bold = ListedColormap(['darkorange', 'c', 'darkblue'])\nfor weights in ['uniform', 'distance']:\n    clf = KNeighborsClassifier(n_neighbors = n_neighbors, weights = weights).fit(X,y)\n    \n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n    \n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n\n    # Put the result into a color plot\n    Z = Z.reshape(xx.shape)\n    plt.figure()\n    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n\n    # Plot also the training points\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold,\n                edgecolor='k', s=20)\n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(), yy.max())\n    plt.title(\"3-Class classification (k = %i, weights = '%s')\"\n              % (n_neighbors, weights))\n    \n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.datasets import load_breast_cancer\ncancer = load_breast_cancer()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, stratify = cancer.target, random_state = 66)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_accuracy = []\ntest_accuracy = []\n\n#try n neighbors from 1 to 10\n\nneighbors_setting = range(1,11)\n\nfor n in neighbors_setting:\n    clf = KNeighborsClassifier(n_neighbors = n)\n    clf.fit(X_train, y_train)\n    #Training set accuracy\n    training_accuracy.append(clf.score(X_train, y_train))\n    #Test set accuracy\n    test_accuracy.append(clf.score(X_test, y_test))\n    \nplt.plot(neighbors_setting, training_accuracy, label = \"Training accuracy\", marker = 'o')\nplt.plot(neighbors_setting, test_accuracy, label = \"Test accuracy\", marker = 'o')\nplt.ylabel(\"Accuracy\")\nplt.xlabel(\"n_neighbors\")\nplt.legend()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsRegressor\nX, y = make_wave(n_samples = 40)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X,y, random_state = 0)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"reg = KNeighborsRegressor(n_neighbors = 3)\nreg.fit(X_train, y_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Test set predictions: \\n{}\".format(reg.predict(X_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Test set R^2: {:.2f}\".format(reg.score(X_test, y_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ns = range(1,11)\ntrain_acc = []\ntest_acc = []\n\nfor n in ns:\n    reg = KNeighborsRegressor(n_neighbors = n)\n    reg.fit(X_train, y_train)\n    train_acc.append(reg.score(X_train, y_train))\n    test_acc.append(reg.score(X_test, y_test))\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(ns, train_acc, label = \"Train_accuracy\")\nplt.plot(ns, test_acc, label = \"Test accuracy\")\nplt.ylabel(\"Accuracy\")\nplt.xlabel(\"n_neighbors\")\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#we can see that our model performs at its best at k = 3","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression \nX,y = make_wave(n_samples = 60)\nX_train, X_test, y_train, y_test = train_test_split(X,y,random_state=  42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LinearRegression()\nlr.fit(X_train, y_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Linear regression coeff-> {}\".format(lr.coef_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Linear regression intercept-> {}\".format(lr.intercept_))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"print(\"Training set score: {:.2f}\".format(lr.score(X_train, y_train))"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Training set score: {:.2f}\".format(lr.score(X_train, y_train)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Test set score: {:.2f}\".format(lr.score(X_test, y_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = lr.predict(X_train)\nplt.plot(X,y,'bo')\nplt.plot(X_train, pred, 'r-')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X,y = load_extended_boston()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X,y, random_state = 0)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LinearRegression().fit(X_train, y_train)\nprint(\"Training set accruacy: {:.2f}\".format(lr.score(X_train, y_train)))\nprint(\"Test set accuracy: {:.2f}\".format(lr.score(X_test, y_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#This discrepancy between training set and test set is a clear indication of overfitting, hence we should try to use regularization \n#linear regression with regularization is called Ridge regression (R2)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Ridge Regression","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Ridge\nridge = Ridge().fit(X_train, y_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Training set score: {:.2f}\".format(ridge.score(X_train, y_train)))\nprint(\"Test set score: {:.2f}\".format(ridge.score(X_test, y_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"range?","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list_of_alphas = []\n\nfor n in np.arange(0,1,0.005):\n    list_of_alphas.append(n)\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_acc = []\ntest_acc = []\n\nfor ele in list_of_alphas:\n    ridge = Ridge(alpha = ele).fit(X_train, y_train)\n    train_acc.append(ridge.score(X_train, y_train))\n    test_acc.append(ridge.score(X_test, y_test))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(list_of_alphas, train_acc, label = 'Train accuracy')\nplt.plot(list_of_alphas, test_acc, label = 'Test accuracy')\nplt.legend()\nplt.ylabel(\"Accuracy\")\nplt.xlabel(\"Alpha in Ridge model\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Lasso ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lasso = Lasso().fit(X_train, y_train)\nprint(\"Training set accuracy = {:.2f}\".format(lasso.score(X_train, y_train)))\nprint(\"Test set accuracy = {:.2f}\".format(lasso.score(X_test, y_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Number of features used = {}\".format(np.sum(lasso.coef_!=0)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lasso1 = Lasso(alpha = 0.01, max_iter = 100000).fit(X_train, y_train)\nprint(\"Training set accuracy = {:.2f}\".format(lasso1.score(X_train, y_train)))\nprint(\"Test set accuracy = {:.2f}\".format(lasso1.score(X_test, y_test)))\nprint(\"Number of features used = {}\".format(np.sum(lasso1.coef_!=0)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lasso2= Lasso(alpha = 0.0001, max_iter = 100000).fit(X_train, y_train)\nprint(\"Training set accuracy = {:.2f}\".format(lasso2.score(X_train, y_train)))\nprint(\"Test set accuracy = {:.2f}\".format(lasso2.score(X_test, y_test)))\nprint(\"Number of features used = {}\".format(np.sum(lasso2.coef_!=0)))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#Setting the alpha value too low will esentially eliminate the presence of lasso regression (or any regularization) "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import LinearSVC","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X,y = make_forge()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(1,2, figsize = (10,3))\n\nfor model, ax in zip([LinearSVC(), LogisticRegression()], axes):\n    clf = model.fit(X,y)\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n    \n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n\n    # Put the result into a color plot\n    Z = Z.reshape(xx.shape)\n    plt.figure()\n    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n\n    # Plot also the training points\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold,\n                edgecolor='k', s=20)\n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(), yy.max())\n    ax.set_title(\"{}\".format(clf.__class__.__name__))\n    ax.set_xlabel(\"feature 0\")\n    ax.set_ylabel(\"feature 1\")\n    plt.legend()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.datasets import load_breast_cancer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings \nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cancer = load_breast_cancer()\nX_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, stratify = cancer.target, random_state = 42)\nlogreg = LogisticRegression().fit(X_train, y_train)\nprint(\"Training set score: {:.3f}\".format(logreg.score(X_train, y_train)))\nprint(\"Test set score: {:.3f}\".format(logreg.score(X_test, y_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logreg100 = LogisticRegression(C = 100).fit(X_train, y_train)\nprint(\"Train set accuracy: {:.3f}\".format(logreg100.score(X_train, y_train)))\nprint(\"Test set accuracy: {:.3f}\".format(logreg100.score(X_test, y_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logreg001 = LogisticRegression(C = 0.001).fit(X_train, y_train)\nprint(\"Train set accuracy: {:.3f}\".format(logreg001.score(X_train, y_train)))\nprint(\"Test set accuracy: {:.3f}\".format(logreg001.score(X_test, y_test)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list_of_c = []\n\nfor n in np.arange(10,20,1):\n    list_of_c.append(n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_acc = []\ntrain_acc = []\n\n\nfor ele in list_of_c:\n    logreg = LogisticRegression(C= ele).fit(X_train, y_train)\n    train_acc.append(logreg.score(X_train, y_train))\n    test_acc.append(logreg.score(X_test, y_test))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_list = []\n\ns1 = pd.Series(train_acc)\ns2 = pd.Series(test_acc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"s3 = s1-s2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for ele in s3: \n    new_list.append((ele))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot?","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, axes = plt.subplots(1,2,figsize = (10,3))\n\naxes[0].plot(list_of_c, train_acc)\naxes[0].plot(list_of_c, test_acc)\n\naxes[1].plot(list_of_c, new_list)\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plotting the coefficients of logistic regression \nplt.figure(figsize = (12,8))\nplt.plot(logreg.coef_.T, 'o', label = \"C= 1\")\nplt.plot(logreg100.coef_.T, '^', label = \"C = 100\")\nplt.plot(logreg001.coef_.T, '*', label = \"C = 0.001\")\nplt.xticks(range(cancer.data.shape[1]), cancer.feature_names, rotation = 90)\nplt.hlines(0,0,cancer.data.shape[1])\nplt.ylim(-5,5)\nplt.xlabel(\"Coefficient index\")\nplt.ylabel(\"Coefficient magnitude\")\nplt.legend()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.datasets import make_blobs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X, y = make_blobs(random_state = 42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (12,8))\ncmap = sns.cubehelix_palette(dark=0, light=.8, as_cmap=True)\nsns.scatterplot(X[:,0],X[:,1], hue = y, style = y, palette = cmap,s = 200) #s = 200 defines the size of the marker here \n#passing X, y, hue (as y), style (change style of markers as y changes), palette = custom defined, s = for size of markers \n\nplt.xlabel(\"Feature 0\", fontsize = 15)\nplt.ylabel(\"Feature 1\", fontsize = 15)\nplt.legend([\"Class 0\", \"Class 1\", \"Class 2\"], fontsize =15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## ****LINEAR SVC****"},{"metadata":{},"cell_type":"markdown","source":"Fitting the LinearSVC classifier on the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"linear_svm =  LinearSVC().fit(X,y)\nprint(\"Coefficient shape: \", linear_svm.coef_.shape)\nprint(\"Intercept shape: \", linear_svm.intercept_.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here, shape of coef_ (3,2) means there are 3 rows (hence three classes in the dataset) and 2 columns meaning 2 features\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (16,10))\nsns.scatterplot(X[:,0], X[:,1], hue = y, style = y, s = 200)\nline = np.linspace(-15,15)\n\nfor coef, intercept, color in zip(linear_svm.coef_, linear_svm.intercept_, ['b','r','g']):\n    plt.plot(line, -(line*coef[0] + intercept)/coef[1], c = color)\n    plt.xlim(-10,15)\n    plt.ylim(-10,10)\n    plt.xlabel(\"Feature 0\", fontsize = 15)\n    plt.ylabel(\"Feature 1\", fontsize = 15)\n    plt.legend(['Line 0', 'Line 1', 'Line 2', 'Class 0', 'Class 1', 'Class 2' ], loc=\"best\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cancer = load_breast_cancer()\nX_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state = 42)\ntree = DecisionTreeClassifier(random_state = 0)\ntree.fit(X_train, y_train)\nprint(\"Training set accuracy : {:.3f}\".format(tree.score(X_train, y_train)))\nprint(\"Test set accuracy: {:.3f}\".format(tree.score(X_test, y_test)))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Perfect training accuracy implies our tree is overfitting the data and needs to be pruned.\n* Hence, we set the max depth (max no. of consecutive questions asked) to 4.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"tree = DecisionTreeClassifier(max_depth = 4, random_state = 0)\ntree.fit(X_train, y_train)\nprint(\"Training set accuracy : {:.3f}\".format(tree.score(X_train, y_train)))\nprint(\"Test set accuracy: {:.3f}\".format(tree.score(X_test, y_test))) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Visualizing the tree ---> import export_graphviz","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import export_graphviz\nexport_graphviz(tree, out_file = \"tree.dot\", class_names = [\"malignant\", \"benign\"], feature_names = cancer.feature_names, impurity = False, filled = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import graphviz\nwith open(\"tree.dot\") as f:\n    dot_graph = f.read()\ngraphviz.Source(dot_graph)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Feature importances: \\n{}\".format(tree.feature_importances_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_feature_importances(model):\n    n_features = cancer.data.shape[1]\n    plt.figure(figsize = (12,8))\n    plt.barh(range(n_features), model.feature_importances_, align = 'center')\n    plt.yticks(np.arange(n_features), cancer.feature_names)\n    plt.xlabel(\"Feature importance\", fontsize = 15)\n    plt.ylabel(\"Feature\", fontsize = 15)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_feature_importances(tree)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}
